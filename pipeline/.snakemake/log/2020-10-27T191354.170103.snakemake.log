Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 12
Job counts:
	count	jobs
	1	align
	1	annotANNOVAR
	1	annotINTERVAR
	1	annotVEP
	1	baseRecalibration
	1	bedToPicardInterval
	3	bgzip
	1	combineGVCF
	1	combineVCF
	2	concatFastq
	1	copyBed
	1	createBaseRecalibrationTable
	1	createGVCFList
	3	depthOfCoverage
	1	downloadGATK
	1	extendBed
	1	extractGNOMAD
	1	extractGNOMADVEP
	2	fastQC
	1	filterIndels
	1	filterSNVs
	1	filtration_freq_and_genes
	1	freqGNOMAD
	1	genotypeGVCF
	1	gunzipCADD
	1	haplotypeCaller
	1	indelRealign
	1	makeAutosomesBed
	1	makeChrXBed
	1	makeChrYBed
	1	markDuplicates
	1	merge
	1	merge_fields
	1	picardMetrics
	1	realignTargetCreator
	1	scoreCADD
	1	selectIndels
	1	selectSNVs
	1	sort
	3	tabix
	1	target
	49

[Tue Oct 27 19:13:54 2020]
rule copyBed:
    input: /sandbox/users/mguillout/pipeline/REFERENCES/Broad.human.exome.b37.bed, /sandbox/users/mguillout/pipeline/REFERENCES/human_g1k_v37.fasta.fai
    output: BED/capture.bed
    jobid: 30
    reason: Missing output files: BED/capture.bed

tr -d '\r' < /sandbox/users/mguillout/pipeline/REFERENCES/Broad.human.exome.b37.bed | grep -v '^browser'  | grep -v '^track' | cut -f1,2,3 | sed 's/^chr//' | LC_ALL=C sort -t '	' -k1,1 -k2,2n -k3,3n | bedtools merge | bedtools sort -faidx /sandbox/users/mguillout/pipeline/REFERENCES/human_g1k_v37.fasta.fai > BED/capture.bed
Submitted job 30 with external jobid 'Your job 4305486 ("snakejob.copyBed.30.sh") has been submitted'.

[Tue Oct 27 19:13:54 2020]
rule align:
    input: /sandbox/users/mguillout/pipeline/REFERENCES/human_g1k_v37.fasta, /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R1_001.fastq.gz, /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R2_001.fastq.gz, /sandbox/users/mguillout/pipeline/REFERENCES/human_g1k_v37.fasta.bwt
    output: Samples/Sample1/BAM/Sample1.p1.sam
    jobid: 42
    reason: Missing output files: Samples/Sample1/BAM/Sample1.p1.sam
    wildcards: sample=Sample1, pairID=p1

bwa mem -t 1 -M 		-H '@CO\tProject:pipelineTest Sample:Sample1 Date:2020-10-27 CWD:/sandbox/users/mguillout/externalshares/Application/pipeline/test_10 Capture:{ name : captureName , description : exome capture 1 , path : /sandbox/users/mguillout/pipeline/REFERENCES/Broad.human.exome.b37.bed}' 		-R '@RG\tID:Sample1\tLB:Sample1\tSM:Sample1\tPL:Illumina\tCN:CENTER' 		/sandbox/users/mguillout/pipeline/REFERENCES/human_g1k_v37.fasta /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R1_001.fastq.gz /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R2_001.fastq.gz > Samples/Sample1/BAM/Sample1.p1.sam
Submitted job 42 with external jobid 'Your job 4305487 ("snakejob.align.42.sh") has been submitted'.

[Tue Oct 27 19:13:54 2020]
rule concatFastq:
    input: /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R2_001.fastq.gz
    output: Samples/Sample1/QC/Sample1.fastqc.reverse.gz
    jobid: 22
    reason: Missing output files: Samples/Sample1/QC/Sample1.fastqc.reverse.gz
    wildcards: sample=Sample1, strand=reverse

cat /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R2_001.fastq.gz > Samples/Sample1/QC/Sample1.fastqc.reverse.gz
Submitted job 22 with external jobid 'Your job 4305488 ("snakejob.concatFastq.22.sh") has been submitted'.

[Tue Oct 27 19:13:54 2020]
rule concatFastq:
    input: /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R1_001.fastq.gz
    output: Samples/Sample1/QC/Sample1.fastqc.forward.gz
    jobid: 21
    reason: Missing output files: Samples/Sample1/QC/Sample1.fastqc.forward.gz
    wildcards: sample=Sample1, strand=forward

cat /sandbox/users/mguillout/fastQ-APHP/P10-BL_S10_L001_R1_001.fastq.gz > Samples/Sample1/QC/Sample1.fastqc.forward.gz
Submitted job 21 with external jobid 'Your job 4305489 ("snakejob.concatFastq.21.sh") has been submitted'.

[Tue Oct 27 19:13:54 2020]
rule downloadGATK:
    output: gatkPresent.txt
    jobid: 17
    reason: Missing output files: gatkPresent.txt

Submitted job 17 with external jobid 'Your job 4305490 ("snakejob.downloadGATK.17.sh") has been submitted'.
Terminating processes on user request, this might take some time.
Will exit after finishing currently running jobs.
Cancelling snakemake on user request.
